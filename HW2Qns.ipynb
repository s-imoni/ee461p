{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdVGS8oSjBkc"
   },
   "source": [
    "## EE 461P: Data Science Principles  \n",
    "### Assignment 2 \n",
    "### Total points: 60\n",
    "### Due: Thursday, Feb 25, 2021, submitted via Canvas by 11:59 pm  \n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
    "\n",
    "### Name(s) and EID(s):\n",
    "1. julian.fritz125@gmail.com\n",
    "2. Simoni Maniar, ssm3256\n",
    "\n",
    "### Homework group No.: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFAfdc7iTrO7"
   },
   "source": [
    "# Question 1 - Cross Validation (15 pts)\n",
    "\n",
    "Use the given code below to load the dataset from \"data.csv\". The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). This dataset was created for energy analysis using 12 different building shapes.The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. By simulating various settings as functions of the afore-mentioned characteristics in total there are 768 building shapes. For more information on the dataset refer this [link](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency). The aim is to use the eight features to predict one of the two responses.In this question, we will predict only the y1 response. \n",
    "\n",
    "Specifically:\n",
    "* X1 - Relative Compactness\n",
    "* X2 - Surface Area\n",
    "* X3 - Wall Area\n",
    "* X4 - Roof Area\n",
    "* X5 - Overall Height\n",
    "* X6 - Orientation\n",
    "* X7 - Glazing Area\n",
    "* X8 - Glazing Area Distribution\n",
    "* y1 - Heating Load\n",
    "* y2 - Cooling Load\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PfGn68OdWQC_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import sklearn\n",
    "\n",
    "data = pd.read_csv(\"data_qn1.csv\",delimiter=\",\")\n",
    "y = data[\"Y1\"]\n",
    "X = data.drop(columns=[\"Y1\",\"Y2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE_2coqAWQZZ"
   },
   "source": [
    "\n",
    "We will be analyzing the following scenarios for the given dataset.\n",
    "\n",
    "* Compare hold-out(80:20) train-test split cross validation and K-Fold Cross Validation \n",
    "* What happens when the number of folds increase for K-Fold Cross Validation? \n",
    "* Variance in the prediction for each case - Hold Out Validation and K-Fold Validation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te7QQxe4XEkh"
   },
   "source": [
    "\n",
    "a) [**3 pts**] Split the original dataset(X,y) into 80:20 [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) split, use linear regression to fit the model on the training data and evaluate the model using the test data. Report the root mean squared error (RMSE) on the test data for five different runs, make sure to store the RMSE values, we will use these values later to plot in part (d).\n",
    "\n",
    "b) [**3 pts**] Now, we will use [K-Fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) validation from sklearn to split the original data(X,y) into 5 folds. For each fold use linear regression to fit the model on training data and evaluate the model on the test data. Compute the average RMSE of the 5 folds, repeat the same for five different random splits of K-Fold. You can refer to the following line of code to perform the split. Make sure to vary the random_state value for five different runs. Record the RMSE values we will use them later in part (d).\n",
    "\n",
    "```\n",
    "kf = KFold(n_splits=5,random_state=random_state,shuffle=True)\n",
    "```\n",
    "\n",
    "c) [**3 pts**] Repeat the same experiment as in part (b) by varying the number of folds as k = 100,768 and record the RMSE for each value of k.\n",
    "\n",
    "d) [**3 pts**] Now, we will plot the box plot of the RMSE values obtained from part a), b) and c) together in a single figure. To reiterate, Hold out Validation(from part a) and k = 5, 100 and 768(part b and c) each with 5 values of RMSE for the 5 random states of the k-fold split. You can refer [here](https://matplotlib.org/stable/gallery/pyplots/boxplot_demo_pyplot.html#sphx-glr-gallery-pyplots-boxplot-demo-pyplot-py) on how to plot the boxplots. Boxplots are used to understand the variance in the values of the RMSE. For more information on box plots refer this [link](https://en.wikipedia.org/wiki/Box_plot). \n",
    "\n",
    "e) [**3 pts**] Using the boxplot answer the following questions,\n",
    "\n",
    "* What do you observe in the variation for RMSE of hold out validation and k-Fold validation, explain with reasoning which one will you choose to evaluate the model.\n",
    "\n",
    "* What happens when the number of folds increase to larger values?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NlllrWv2JMJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrn3rt3wzMv7"
   },
   "source": [
    "# Question 2 - Bias Variance TradeOff (10 pts)\n",
    "\n",
    "a) [**5pts**] What is the difference between the notion of model bias (for example, a model that predicts age as a function of some other features) and the bias of a point estimator (for example, the mean age of students estimated from a sample of age values)?\n",
    "\n",
    "b) [**5pts**] a) Assume you have a model trained to solve a problem. How do you expect (i) the bias and (ii) the variance to change if you used a larger training dataset, but no other process changed?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaPpQFaiB39a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT0qzWQHB4RT"
   },
   "source": [
    "# Question 3: Ridge and Lasso Regression (15 points)\n",
    "\n",
    "In this question you will explore the application of Lasso and Ridge regression using sklearn package in Python. Use the following code to load the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "twwroG3-P4fi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"data_qn3.csv\", index_col=0)\n",
    "df = df.loc[df['Year']==2014, :]\n",
    "df = df.drop('Year', axis=1)\n",
    "df = pd.get_dummies(df, columns=['Status'])\n",
    "df = df.dropna()\n",
    "\n",
    "# Creating training and testing dataset\n",
    "y = df.iloc[:, 0]\n",
    "X = df.iloc[:, 1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHdoWCdqQDHz"
   },
   "source": [
    "## Question 3.1 (3 points) \n",
    "Run Linear regression on the train dataset and print the $R^2$ value using the test dataset.\n",
    "\n",
    "## Question 3.2 (6 points) \n",
    "Run linear regression using Lasso and determine the value of $\\alpha$ that results in best test set performance. Consider `alphas=10**np.linspace(1,-2,100)*0.5`. Display the best value of $\\alpha$ as well as the corresponsing $R^2$ score on test set. Use the following parameters in Lasso model. Finally, store the best model separately. Also, use the co-efficients obtained and select the [columns with non-zero weights](https://stackoverflow.com/questions/62323713/selecting-columns-of-dataframe-where-lasso-coefficient-is-nonzero) and use them to create `X_train_lasso` and `X_test_lasso`. Show how many non-zero columns are present. Plot the coefficients of the lasso model for different alpha values, you can use log scale to plot the alphas in the x_axis.\n",
    "\n",
    "    copy_X=True\n",
    "    normalize=True # Normalizes data using StandardScaler()\n",
    "    random_state=42\n",
    "\n",
    "## Question 3.3 (6 points) \n",
    "Run linear regression using Ridge and determine the value of $\\alpha$ that results in best test set performance. Consider `alphas=10**np.linspace(1,-2,100)*0.5`. Display the best value of $\\alpha$ as well as the corresponsing $R^2$ score on test set. Use the following parameters in Ridge model.Plot the coefficients of the ridge model for different alpha values, you can use log scale to plot the alphas in the x_axis.\n",
    "\n",
    "    copy_X=True\n",
    "    normalize=True # Normalizes data using StandardScaler()\n",
    "    random_state=42\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xzD571z_3hSz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination using test: 0.8355055233743603\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "from sklearn.metrics import r2_score\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = reg.predict(X_test)\n",
    "print('Coefficient of determination using test:', r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b4abef30559a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0my_pred_lasso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mcurrent_r2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_lasso\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mcoef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_r2\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mcurrent_r2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0malpha_for_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2\n",
    "from sklearn.linear_model import Lasso\n",
    "alpha=10**np.linspace(1,-2,100)*0.5\n",
    "max_r2 = 0;\n",
    "plt.figure(0)\n",
    "plt.title(\"Lasso coefficients for each alpha\");\n",
    "plt.xlabel(\"Alphas\");\n",
    "plt.ylabel(\"Coefficients\");\n",
    "plt.xscale('log')\n",
    "coef = np.array([])\n",
    "alpha_for_max = 0;\n",
    "for x in alpha:\n",
    "    reg2 = Lasso(alpha=x, copy_X=True, normalize=True, random_state=42)\n",
    "    reg2.fit(X_train, y_train)\n",
    "    y_pred_lasso = reg2.predict(X_test);\n",
    "    current_r2 = r2_score(y_test, y_pred_lasso);\n",
    "    coef.append(reg2.coef_)\n",
    "    if(max_r2 < current_r2):\n",
    "        alpha_for_max = x;\n",
    "        max_r2 = current_r2;   \n",
    "\n",
    "print('Maximum Coefficient of determination using LASSO:', max_r2);\n",
    "print('Best value of Alpha:', alpha_for_max)\n",
    "\n",
    "best_model = Lasso(alpha=alpha_for_max, copy_X=True, normalize=True, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Also, use the co-efficients obtained and select the columns with non-zero weights and use them to create X_train_lasso and X_test_lasso. \n",
    "X_train_lasso = X_train.iloc[:,best_model.coef_!=0]\n",
    "X_test_lasso = X_test.iloc[:,best_model.coef_!=0]\n",
    "\n",
    "plt.plot(alpha, coef);\n",
    "\n",
    "# Show how many non-zero columns are present. \n",
    "print(\"Shape to show 11 non-zero columns of X_train_lasso:\", X_train_lasso.shape)\n",
    "print(\"Shape to show 11 non-zero columns of X_test_lasso:\", X_test_lasso.shape)\n",
    "\n",
    "# Plotted coefficients of lasso model with different alpha values in for loop above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3\n",
    "from sklearn.linear_model import Ridge\n",
    "alpha=10**np.linspace(1,-2,100)*0.5\n",
    "max_r2 = 0;\n",
    "plt.figure(0)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Lasso coefficients for each alpha\");\n",
    "plt.xlabel(\"Alphas\");\n",
    "plt.ylabel(\"Coefficients\");\n",
    "coef = []\n",
    "alpha_for_max = 0;\n",
    "for x in alpha:\n",
    "    reg2 = Ridge(alpha=x, copy_X=True, normalize=True, random_state=42)\n",
    "    reg2.fit(X_train, y_train)\n",
    "    y_pred_ridge = reg2.predict(X_test);\n",
    "    current_r2 = r2_score(y_test, y_pred_ridge);\n",
    "    coef.append(reg2.coef_)\n",
    "    if(max_r2 < current_r2):\n",
    "        alpha_for_max = x;\n",
    "        max_r2 = current_r2;   \n",
    "\n",
    "print('Maximum Coefficient of determination using RIDGE:', max_r2);\n",
    "print('Best value of Alpha:', alpha_for_max)\n",
    "plt.plot(alpha, coef);\n",
    "\n",
    "# Plotted coefficients of ridge model with different alpha values in for loop above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzPjp2hd9qkH"
   },
   "source": [
    "# Question 4: Polynomial Feature Transformation (20 points) \n",
    "Often, you will find that transforming features into higher degrees will yield better models. In this question, we will see how to do non-linear regression using a linear model by using polynomial feature transformation. You will need to build only one plot for this entire question. So, plot everything on the same plot. Let us now consider the following dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_jfDHNpL-0i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "def h(x):\n",
    "    \"\"\" function to approximate by polynomial interpolation\"\"\"\n",
    "    return np.sin(x) + np.log(x) \n",
    "\n",
    "\n",
    "# generate points used to plot\n",
    "x_plot = np.linspace(2, 12, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "x = np.linspace(2, 12, 100)\n",
    "\n",
    "rng = np.random.RandomState(20)\n",
    "rng.shuffle(x)\n",
    "\n",
    "x_train = np.sort(x[:50])\n",
    "x_test = np.sort(x[50:80])\n",
    "\n",
    "\n",
    "# create matrix versions of these arrays\n",
    "x_train = x_train[:, np.newaxis]\n",
    "x_test = x_test[:,np.newaxis]\n",
    "x_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "y_train = h(x_train) + np.random.normal(0, 0.5, size=x_train.shape) \n",
    "y_test = h(x_test)+ np.random.normal(0, 0.5, size=x_test.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoQFk35CL-Bl"
   },
   "source": [
    "1. Build a scatter plot with `s=30` and `marker='o'` using x_train and y_train. Also, build a line plot using `x_plot` and `h(x_plot)` to show the trend. (5 pts)\n",
    "2. Transform `x_train` and `x_test` using [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) with degrees 1,3,5,7,9,11 and save these transformed datasets. For example, if an input sample is two dimensional and of the form [$a$, $b$], the degree-2 polynomial features are [$1$, $a$, $b$, $a^2$, $ab$, $b^2$]. (5 pts)\n",
    "3. Use ridge regression with default parameters on each of these train datasets. Now, calculate the predicted target values for the fitted model using `.predict(X_plot)` and show line plots using `x_plot` and the predicted target values. Also, calculate the training MSE and test MSE for each of them using the model. (5 pts)\n",
    "4. Report your observations from the plot w.r.t how the evaluation metrics change on increasing the `degree` parameter. What do you think will happen if we keep on increasing the value of `degree`? (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMJ_ZadzRGkG"
   },
   "outputs": [],
   "source": [
    "#4.1\n",
    "plt.figure(0)\n",
    "plt.title('Scatter Plot')\n",
    "plt.scatter(x_train, y_train, s=30, marker='o')\n",
    "plt.figure(1)\n",
    "plt.title('Line Plot')\n",
    "plt.plot(x_plot, h(x_plot))\n",
    "plt.figure(2)\n",
    "plt.title('Scatter Plot and Line Plot superimposed')\n",
    "plt.scatter(x_train, y_train, s=30, marker='o')\n",
    "plt.plot(x_plot, h(x_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2\n",
    "#degrees 1,3,5,7,9,11 \n",
    "\n",
    "#degree 1;\n",
    "x_test_1 = PolynomialFeatures(1)\n",
    "x_test_1_transformed = x_test_1.fit_transform(x_test)\n",
    "x_train_1 = PolynomialFeatures(1)\n",
    "x_train_1_transformed = x_train_1.fit_transform(x_train)\n",
    "\n",
    "#degree 3;\n",
    "x_test_3 = PolynomialFeatures(3)\n",
    "x_test_3_transformed = x_test_3.fit_transform(x_test)\n",
    "x_train_3 = PolynomialFeatures(3)\n",
    "x_train_3_transformed = x_train_3.fit_transform(x_train)\n",
    "\n",
    "#degree 5;\n",
    "x_test_5 = PolynomialFeatures(5)\n",
    "x_test_5_transformed = x_test_5.fit_transform(x_test)\n",
    "x_train_5 = PolynomialFeatures(5)\n",
    "x_train_5_transformed = x_train_5.fit_transform(x_train)\n",
    "\n",
    "#degree 7;\n",
    "x_test_7 = PolynomialFeatures(7)\n",
    "x_test_7_transformed = x_test_7.fit_transform(x_test)\n",
    "x_train_7 = PolynomialFeatures(7)\n",
    "x_train_7_transformed = x_train_7.fit_transform(x_train)\n",
    "\n",
    "#degree 9;\n",
    "x_test_9 = PolynomialFeatures(9)\n",
    "x_test_9_transformed = x_test_9.fit_transform(x_test)\n",
    "x_train_9 = PolynomialFeatures(9)\n",
    "x_train_9_transformed = x_train_9.fit_transform(x_train)\n",
    "\n",
    "#degree 11;\n",
    "x_test_11 = PolynomialFeatures(11)\n",
    "x_test_11_transformed = x_test_11.fit_transform(x_test)\n",
    "x_train_11 = PolynomialFeatures(11)\n",
    "x_train_11_transformed = x_train_11.fit_transform(x_train);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.3\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "# og\n",
    "reg = Ridge();\n",
    "reg.fit(x_train, y_train);\n",
    "y_predict_test = reg.predict(x_test);\n",
    "y_predict_train = reg.predict(x_train);\n",
    "print(\"MSE using train on Original:\", mean_squared_error(y_train, y_predict_train))\n",
    "print(\"MSE using test on Original:\", mean_squared_error(y_test, y_predict_test))\n",
    "plt.figure(0)\n",
    "plt.title(\"Original\")\n",
    "plt.plot(x_plot[50:80], y_predict_test, label=\"test\");\n",
    "plt.plot(x_plot[:50], y_predict_train, label=\"train\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "reg_1 = Ridge();\n",
    "reg_1.fit(x_train_1_transformed, y_train);\n",
    "y_predict_1_test = reg_1.predict(x_test_1_transformed);\n",
    "y_predict_1_train = reg_1.predict(x_train_1_transformed);\n",
    "print(\"MSE using train on 1 degree:\", mean_squared_error(y_train, y_predict_1_train))\n",
    "print(\"MSE using test on 1 degree:\", mean_squared_error(y_test, y_predict_1_test))\n",
    "plt.figure(1)\n",
    "plt.title(\"1 Degree\")\n",
    "plt.plot(x_plot[50:80], y_predict_1_test, label=\"test\");\n",
    "plt.plot(x_plot[:50], y_predict_1_train, label=\"train\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "reg_3 = Ridge();\n",
    "reg_3.fit(x_train_3_transformed, y_train);\n",
    "y_predict_3_test = reg_3.predict(x_test_3_transformed);\n",
    "y_predict_3_train = reg_3.predict(x_train_3_transformed);\n",
    "print(\"MSE using train on 3 degree:\", mean_squared_error(y_train, y_predict_3_train))\n",
    "print(\"MSE using test on 3 degree:\", mean_squared_error(y_test, y_predict_3_test))\n",
    "plt.figure(2)\n",
    "plt.title(\"3 Degree\")\n",
    "plt.plot(x_plot[50:80], y_predict_3_test, label=\"test\");\n",
    "plt.plot(x_plot[:50], y_predict_3_train, label=\"train\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "reg_5 = Ridge();\n",
    "reg_5.fit(x_train_5_transformed, y_train);\n",
    "y_predict_5_test = reg_5.predict(x_test_5_transformed);\n",
    "y_predict_5_train = reg_5.predict(x_train_5_transformed);\n",
    "print(\"MSE using train on 5 degree:\", mean_squared_error(y_train, y_predict_5_train))\n",
    "print(\"MSE using test on 5 degree:\", mean_squared_error(y_test, y_predict_5_test))\n",
    "plt.figure(3)\n",
    "plt.title(\"5 Degree\")\n",
    "plt.plot(x_plot[50:80], y_predict_5_test, label=\"test\");\n",
    "plt.plot(x_plot[:50], y_predict_5_train, label=\"train\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "reg_7 = Ridge();\n",
    "reg_7.fit(x_train_7_transformed, y_train);\n",
    "y_predict_7_test = reg_7.predict(x_test_7_transformed);\n",
    "y_predict_7_train = reg_7.predict(x_train_7_transformed);\n",
    "print(\"MSE using train on 7 degree:\", mean_squared_error(y_train, y_predict_7_train))\n",
    "print(\"MSE using test on 7 degree:\", mean_squared_error(y_test, y_predict_7_test))\n",
    "plt.figure(4)\n",
    "plt.title(\"7 Degree\")\n",
    "plt.plot(x_plot[50:80], y_predict_7_test, label=\"test\");\n",
    "plt.plot(x_plot[:50], y_predict_7_train, label=\"train\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "reg_9 = Ridge();\n",
    "reg_9.fit(x_train_9_transformed, y_train);\n",
    "y_predict_9_test = reg_9.predict(x_test_9_transformed);\n",
    "y_predict_9_train = reg_9.predict(x_train_9_transformed);\n",
    "print(\"MSE using train on 9 degree:\", mean_squared_error(y_train, y_predict_9_train))\n",
    "print(\"MSE using test on 9 degree:\", mean_squared_error(y_test, y_predict_9_test))\n",
    "plt.figure(5)\n",
    "plt.title(\"9 Degree\")\n",
    "plt.plot(x_plot[50:80], y_predict_9_test, label=\"test\");\n",
    "plt.plot(x_plot[:50], y_predict_9_train, label=\"train\");\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "reg_11 = Ridge();\n",
    "reg_11.fit(x_train_11_transformed, y_train);\n",
    "y_predict_11_test = reg_11.predict(x_test_11_transformed);\n",
    "y_predict_11_train = reg_11.predict(x_train_11_transformed);\n",
    "print(\"MSE using train on 11 degree:\", mean_squared_error(y_train, y_predict_11_train))\n",
    "print(\"MSE using test on 11 degree:\", mean_squared_error(y_test, y_predict_11_test))\n",
    "plt.figure(6)\n",
    "plt.title(\"11 Degree\")\n",
    "plt.plot(x_plot[50:80], y_predict_11_test, label=\"test\");\n",
    "plt.plot(x_plot[:50], y_predict_11_train, label=\"train\");\n",
    "plt.legend(loc=\"upper left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: The MSE for the train data set is monotonically decreasing as we add more degrees to the polynomial. This is because as you add more degrees to your polynomial it becomes easier to fit all of the data exactly. However, the MSE for the test data set is increasing. This indicicates that while a higher degreed polynomial might fit the data that is given, it is not the best fit for the overall model. \n",
    "\n",
    "What will happen if we keep increasing degree: The mean squared error for test will continue to increase while the mean squared error for train will continue to decrase. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW2Qns.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
