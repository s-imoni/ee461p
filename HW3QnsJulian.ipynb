{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzWJ0hrg7z77"
   },
   "source": [
    "## EE 461P: Data Science Principles  \n",
    "### Assignment 3 \n",
    "### Total points: 75\n",
    "### Due: Thursday, Mar 11, 2021, submitted via Canvas by 11:59 pm  \n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
    "\n",
    "### Name(s) and EID(s):\n",
    "1. Julian Fritz jjf2459\n",
    "2. \n",
    "\n",
    "### Homework group No.: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fce1WkbMOWRC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGkiu-dcHHi3"
   },
   "source": [
    "# Question 1 : Tensor Playground (15 points)\n",
    "Visit https://playground.tensorflow.org for this problem\n",
    "\n",
    "**NOTE** : Make sure to attach screen shots of your network,train and test loss, parameters in the notebook. \n",
    " \n",
    "From the far right, select \"Classification\" as the problem type, and select the data set on the top right (alternate colored squares).\n",
    "\n",
    "Use these settings as the DEFAULT settings for all subquestions: test/training ratio = 50%, Noise = 0, Batch Size = 30, learning rate = 0.03, activation = tanh, one hidden layer with 2 neurons, input as $X_1$, $X_2$ and no Regularization.\n",
    "\n",
    "1) Use the DEFAULT setting and run two experiments - one using Relu as the activation function and one using the linear activation function. Report the train, test losses for both these experiments at the end of 1000 epochs. What  do you observe in the decision boundaries obtained? Why? (**3 pts**)\n",
    "\n",
    "2)Using the same DEFAULT setting as above, increase the number of neurons to 3 train the network multiple times(5-10 times), to reset the network use the RESET button left of the PLAY button. What do you observe in the loss values, as well as the decision boundary across the different training runs ? Could you say the training is stable(uniform) across the mutliple runs, if not why. Provide reasoning (**3 pts**)\n",
    "\n",
    "We will now study the effect of certain variations in the network structure or training, keeping all other aspects the same as in the default configuration specified above, and with tanh as the activation for hidden units.\n",
    "\n",
    "3) ```Effect of number of hidden units```: Go back to DEFAULT settings and report the train, test losses at the end of 1000 epochs for 4 and 8 neurons in the hidden layer (2 pairs of results). What do you observe regarding the decision boundary as the number of neurons increases? Why? (**3 pts**)\n",
    "\n",
    "4) ```Effect of learning rate and number of epochs```: Go back to DEFAULT settings and set the activation to be Relu and have four neurons in the hidden layer. Report the train, test losses at the end of 100 epochs and 1000 epochs for learning rates of 10, 0.1, 0.01 and 0.001 (8 pairs of results). What do you observe in the loss curves? Explain. (**3 pts**)\n",
    "\n",
    "5)Use the same default setting, if you are asked to vary the number of neurons or number of hidden layers for this problem and train the network for 500 epochs with all the other settings constant. i.e you can change only the number of neurons or number of hidden layers which decision you would take, to reduce the test loss (test loss should be at most 0.06). Justify your decisions\n",
    "and briefly comment on difficulties/tradeoffs, what helps/what doesn't, etc. (**3 pts**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sYj8TlJOZsN"
   },
   "source": [
    "# Answer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_662n1r6PWT7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El0xfptHTjty"
   },
   "source": [
    "# Question 2 - Stochastic Gradient Descent (25 pts)\n",
    "1. (**5pts**) Using stochastic gradient descent, derive the coefficent updates for all coefficients of the model (assuming squared error as your loss function): \n",
    "$$ y = w_0 + w_1x_1 + w_2x_1^{2}x_2 + w_3e^{-x_1}+ w_4log(x_3) $$ \n",
    "\n",
    "\n",
    "2. (**20pts**) Write Python code for an SGD solution to the non-linear model$$ y = w_0 + w_1x_1 + w_2x_1^{2}x_2 + w_3e^{-x_1}+ w_4log(x_3) $$   Try to format similarly to scikit-learn's models. The template of the class is given. The init function of the class takes as input the learning_rate, regularization_constant and number of epochs. The fit method must take as input X,y. The _predict_ method takes an X value (optionally, an array of values). \n",
    "\n",
    "  **a**) Use your new gradient descent regression to predict the data given in ```'samples.csv'```, for 15 epochs, using learning rates: ```[0, .0001, .001, .01, 0.1, 1, 10, 100]``` and regularization (ridge regression) parameters: ```[0,10,100]```. (**13pts**)\n",
    "\n",
    "  **b**) Plot MSE and the $w$ parameters as a function of epoch (for 15 epochs) for the two best combinations of learning_rate and regularization for SGD. Here \"best\" means lowest MSE at the end of 15 epochs. (**5pts**)\n",
    "\n",
    "  ```NOTE``` : In this setting there is no validation/test data, and stopping after a pre-determined number of epochs is an example of an \"early stopping\" approch to avoid overfitting.(This approach is not ideal, but is sometimes employed in the absence of a validation dataset).\n",
    "\n",
    "  **c**) Also report the MSE at the end of 15 epochs that you obtained for these two \"best\" combinations. (**2pts**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTalYijUOCdP"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Regression:\n",
    "    \n",
    "    def __init__(self, learning_rate, regularization, n_epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoch = n_epoch\n",
    "        self.regularization = regularization\n",
    "        \n",
    "    def sgd(self, gradient):\n",
    "        self.coef # = please fill this to update self.coef using SGD\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y, update_rule='sgd', plot=False):\n",
    "        mse = []\n",
    "        coefs = []\n",
    "        X = self.get_features(X)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for i in range(X.shape[0]):\n",
    "                # Compute error\n",
    "                   #please fill this\n",
    "                # Compute gradients\n",
    "                    #please fill this\n",
    "               \n",
    "                # Update weights\n",
    "                self.sgd(gradient)\n",
    "\n",
    "            coefs.append(self.coef)\n",
    "            residuals = y - self.linearPredict(X)         \n",
    "            mse.append(np.mean(residuals**2))\n",
    "            \n",
    "        self.lowest_mse = mse[-1]\n",
    "        if plot == True:\n",
    "            plt.figure()\n",
    "            plt.plot(range(self.n_epoch),mse)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.figure()\n",
    "            coefs = np.array(coefs)\n",
    "            plt.plot(range(self.n_epoch),coefs[:,0],label='w0')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,1],label='w1')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,2],label='w2')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,3],label='w3')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,4],label='w4')\n",
    "            plt.legend()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('parameter value')\n",
    "\n",
    "    def get_features(self, X):\n",
    "        '''\n",
    "        this output of this function can be used to compute the gradient in `fit`\n",
    "        '''\n",
    "        x = np.zeros((X.shape[0], 5))\n",
    "        x[:,0] = 1\n",
    "        x[:,1] = (X[:,0])\n",
    "        x[:,2] = (X[:,0]**2)*X[:,1]\n",
    "        x[:,3] = np.exp(-X[:,0])\n",
    "        x[:,4] = np.log(X[:,2])\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    #def linearPredict(self, X):  \n",
    "        #compute dot product of self.coef and X\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "2O9WiylTODsc",
    "outputId": "b7aa78cf-ab97-4ce2-ba2e-498929c40f80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050816</td>\n",
       "      <td>0.295442</td>\n",
       "      <td>0.305858</td>\n",
       "      <td>2.517320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038277</td>\n",
       "      <td>0.606797</td>\n",
       "      <td>0.855657</td>\n",
       "      <td>2.537360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.761375</td>\n",
       "      <td>0.421169</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>2.047563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.763610</td>\n",
       "      <td>0.537137</td>\n",
       "      <td>0.820876</td>\n",
       "      <td>2.095084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.525309</td>\n",
       "      <td>0.859292</td>\n",
       "      <td>0.161640</td>\n",
       "      <td>2.164664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.396302</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.291709</td>\n",
       "      <td>2.166131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.204226</td>\n",
       "      <td>0.393554</td>\n",
       "      <td>0.828117</td>\n",
       "      <td>2.313066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.970607</td>\n",
       "      <td>0.512186</td>\n",
       "      <td>0.109008</td>\n",
       "      <td>2.161382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.503045</td>\n",
       "      <td>0.877230</td>\n",
       "      <td>0.816781</td>\n",
       "      <td>2.169543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.244220</td>\n",
       "      <td>0.614174</td>\n",
       "      <td>0.110920</td>\n",
       "      <td>2.279306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           x1        x2        x3         y\n",
       "0    0.050816  0.295442  0.305858  2.517320\n",
       "1    0.038277  0.606797  0.855657  2.537360\n",
       "2    0.761375  0.421169  0.232194  2.047563\n",
       "3    0.763610  0.537137  0.820876  2.095084\n",
       "4    0.525309  0.859292  0.161640  2.164664\n",
       "..        ...       ...       ...       ...\n",
       "495  0.396302  0.610169  0.291709  2.166131\n",
       "496  0.204226  0.393554  0.828117  2.313066\n",
       "497  0.970607  0.512186  0.109008  2.161382\n",
       "498  0.503045  0.877230  0.816781  2.169543\n",
       "499  0.244220  0.614174  0.110920  2.279306\n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"samples.csv\",index_col=0)\n",
    "X = np.array([df['x1'].values, df['x2'].values,df['x3'].values]).T\n",
    "y = df['y'].values\n",
    "n_epochs = 15\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "regularization = [0, 10, 100]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYLeKkn88Byc"
   },
   "source": [
    "# Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvRFbXRFWq1y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29K6H6feWqv8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eHA14kQxPgr"
   },
   "source": [
    "# Question 3: Compute the weights (20 pts)\n",
    "\n",
    "For the given network, $i$, $W$ and $O$ represent the inputs, weights and output respectively and let $l$ be the learning rate.\n",
    "\n",
    "Activation: Logistic\n",
    "\n",
    "Loss: Squared Loss\n",
    "\n",
    "Q: Derive the equations to update the values of $w5$ and $w1$ after one iteration of backpropagation.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1GvqRyv_cgS9xIcElF67UtlGoZyHPCHzL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJX6uV03W1uZ"
   },
   "source": [
    "# Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZZ3WXjhW5JS"
   },
   "source": [
    "### For $w5$:  \n",
    "\n",
    "$$ w5^* = w5 - l * \\frac{\\delta Error}{\\delta w5}  $$   \n",
    "\n",
    "Chain Rule:\n",
    "$$ \\frac{\\delta Error}{\\delta w5} = \\frac{\\delta Error}{\\delta O} * \\frac{\\delta O}{\\delta w5} $$     \n",
    "\n",
    "$$ \\frac{\\delta Error}{\\delta O} = \\frac{\\delta (O - target)^2}{\\delta O} = 2 (O - target)    $$   \n",
    "\n",
    "O is our output, following the diagram and using logistic activation function on h1, h2 and h3:   \n",
    "$$ O = \\frac{1}{1 + e^{-(i1w1+i2w3)}} * w5 + \\frac{1}{1 + e^{-(i1w7+i2w8)}} + \\frac{1}{1 + e^{-(i1w2+i2w4)}} * w6  $$ \n",
    "\n",
    "Therefore:\n",
    "$$ \\frac{\\delta O}{\\delta w5} =  \\frac{1}{1 + e^{-(i1w1+i2w3)}}  $$    \n",
    "\n",
    "Plugging them in:\n",
    "$$ \\frac{\\delta Error}{\\delta w5} = 2 (O - target) * \\frac{1}{1 + e^{-(i1w1+i2w3)}} $$     \n",
    "\n",
    "Plugging that into the first equation:\n",
    "$$ w5^* = w5 - 2l * (O - target) * \\frac{1}{1 + e^{-(i1w1+i2w3)}}  $$     \n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now for $w1$:\n",
    "$$ w1^* = w1 - l * \\frac{\\delta Error}{\\delta w1}  $$ \n",
    "\n",
    "Chain Rule:\n",
    "$$ \\frac{\\delta Error}{\\delta w1} = \\frac{\\delta Error}{\\delta O} * \\frac{\\delta O}{\\delta w1} $$\n",
    "\n",
    "Once again:\n",
    "$$ \\frac{\\delta Error}{\\delta O} = \\frac{\\delta (O - target)^2}{\\delta O} = 2 (O - target)    $$\n",
    "\n",
    "Once again:   \n",
    "$$ O = \\frac{1}{1 + e^{-(i1w1+i2w3)}} * w5 + \\frac{1}{1 + e^{-(i1w7+i2w8)}} + \\frac{1}{1 + e^{-(i1w2+i2w4)}} * w6  $$\n",
    "\n",
    "Taking partial derivative w.r.t. $w1$ this time:\n",
    "$$ \\frac{\\delta O}{\\delta w1} =  \\frac{w5*i1*e^{-(i1w1+i2w3)}}{(1 + e^{-(i1w1+i2w3)})^2}  $$\n",
    "\n",
    "Plugging them in:\n",
    "$$ \\frac{\\delta Error}{\\delta w1} = 2 (O - target) * \\frac{w5*i1*e^{-(i1w1+i2w3)}}{(1 + e^{-(i1w1+i2w3)})^2} $$ \n",
    "\n",
    "Plugging that into the first equation:\n",
    "$$ w1^* = w1 - 2l * (O - target) * \\frac{w5*i1*e^{-(i1w1+i2w3)}}{(1 + e^{-(i1w1+i2w3)})^2}  $$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbjhLbwz2rsp"
   },
   "source": [
    "# Question 4 - Multi-layer perceptron Regressor (15 pts)\n",
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python;\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html.\n",
    "\n",
    "\n",
    "We will use the OpenCL gemm kernel performance prediction dataset for this problem; https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance.\n",
    "\n",
    "The following code will pre-process the data and split the data into training and test sets using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 30** and **test_size = 0.25**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJDvB_Q-xaay",
    "outputId": "02a75a78-efdc-446b-93d0-d47e6f65b416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181200, 14) (181200,) (60400, 14) (60400,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('sgemm_product.csv') \n",
    "data['target'] = 0.25*(data['Run1 (ms)'] + data['Run2 (ms)']+ data['Run3 (ms)'] + data['Run4 (ms)'])\n",
    "y = data['target']\n",
    "y = y.to_numpy()\n",
    "X = data.drop(['target','Run1 (ms)', 'Run2 (ms)', 'Run3 (ms)', 'Run4 (ms)'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state=30)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_QeLtnD3eLT"
   },
   "source": [
    "We also want to use [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on the original data, we use StandardScaler to center each feature ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Also remember that when we have training and testing data, we fit preprocessing parameters on training data and apply them to all testing data. You should scale only the features (independent variables), not the target variable y.\n",
    "\n",
    "1) (5pts) Use [sklearn.neural_nework.MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to do a 5-fold cross validation using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). The cross validation must be performed on the **training data**. Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=30, learning_rate_init = 0.005\n",
    "    \n",
    "Now, build four MLP Regressors containing one hidden layer with 2, 10, 20 and 30 hidden units respectively. To do that in scikit-learn, the following syntax needs to be used:\n",
    "    \n",
    "   (a) *hidden_layer_sizes = (2,)* \n",
    "   \n",
    "   (b) *hidden_layer_sizes = (10,)*\n",
    "   \n",
    "   (c) *hidden_layer_sizes = (20,)*\n",
    "   \n",
    "   (d) *hidden_layer_sizes = (30,)*\n",
    "   \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model.\n",
    "   \n",
    "   \n",
    "2) (5pts) Now, using the same parameters used in part 1), train MLPRegressor models on the entire training set and report the RMSE score for both the training and testing sets (again, use StandardScaler). Which of the four models ((a)-(d)) performs the best? Briefly analyze and discuss the results, commenting on the number of hidden units.\n",
    "\n",
    "\n",
    "3) (5pts) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each epoch (misleadingly referred to as \"iteration\" in scikit documentation, though they use epoch in the actual code!). For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot three curves using the following three conditions (a, b, c) in one figure, where *X-axis* is epoch  number and *Y-axis* is squared root of *loss\\_curve\\_*:\n",
    "\n",
    "   (a) *hidden_layer_sizes = (1,)* \n",
    "   \n",
    "   (b) *hidden_layer_sizes = (5,)*\n",
    "   \n",
    "   (c) *hidden_layer_sizes = (10,)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeNBzmHc7kC3"
   },
   "source": [
    "# Answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 229.8053295764077 for hidden layer size 2\n",
      "RMSE is 178.55643955902715 for hidden layer size 10\n",
      "RMSE is 181.09629087164166 for hidden layer size 20\n",
      "RMSE is 153.46570494089562 for hidden layer size 30\n"
     ]
    }
   ],
   "source": [
    "# 1)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "hl_sizes = [2, 10, 20, 30]\n",
    "for i in hl_sizes:\n",
    "    total_rmse = 0\n",
    "    for train_index, test_index in kf.split(X_train_scaled):\n",
    "        X_train_fold, X_test_fold = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        MLPreg = MLPRegressor(hidden_layer_sizes=(i,), activation = 'tanh', solver = 'sgd', learning_rate='constant', \n",
    "                          random_state=42, batch_size=30, learning_rate_init = 0.005).fit(X_train_fold,y_train_fold)\n",
    "        \n",
    "        rmse = mean_squared_error(y_test_fold, MLPreg.predict(X_test_fold), squared=False)\n",
    "        total_rmse += rmse\n",
    "    avg_rmse = total_rmse / n_splits\n",
    "    print(\"RMSE is {} for hidden layer size {}\".format(avg_rmse,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ps1v9Jx26CGE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE is 211.4572621223227 for hidden layer size 2\n",
      "Train RMSE is 215.73189791612663 for hidden layer size 2\n",
      "Test RMSE is 145.23367136134348 for hidden layer size 10\n",
      "Train RMSE is 149.17818926125327 for hidden layer size 10\n",
      "Test RMSE is 205.99829777415934 for hidden layer size 20\n",
      "Train RMSE is 209.52739324613103 for hidden layer size 20\n",
      "Test RMSE is 173.18066043259884 for hidden layer size 30\n",
      "Train RMSE is 177.58002027206086 for hidden layer size 30\n"
     ]
    }
   ],
   "source": [
    "# 2)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "hl_sizes = [2, 10, 20, 30]\n",
    "for i in hl_sizes:\n",
    "    MLPreg = MLPRegressor(hidden_layer_sizes=(i,), activation = 'tanh', solver = 'sgd', learning_rate='constant', \n",
    "                          random_state=42, batch_size=30, learning_rate_init = 0.005).fit(X_train_scaled,y_train)\n",
    "    rmse_test = mean_squared_error(y_test, MLPreg.predict(X_test_scaled), squared=False)\n",
    "    rmse_train = mean_squared_error(y_train, MLPreg.predict(X_train_scaled), squared=False)\n",
    "    print(\"Test RMSE is {} for hidden layer size {}\".format(rmse_test,i))\n",
    "    print(\"Train RMSE is {} for hidden layer size {}\".format(rmse_train,i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer size 10 (model b) performed the best, but it's interesting that the error went down from model a to b, then it went up from b to c, then back down from c to d. Clearly increasing the number of hidden units does not appear to have a monotonic effect on the test/train error. Also worth noting that the Train RMSE was ~4 more than the Test RMSE for every model, which seems like a strange coincidence, and also the fact that the training error was larger than the testing error could mean our models are underfitting a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#3)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "hl_sizes = [1, 5, 10]\n",
    "for i in hl_sizes:\n",
    "    MLPreg = MLPRegressor(hidden_layer_sizes=(i,), activation = 'tanh', solver = 'sgd', learning_rate='constant', \n",
    "                          random_state=42, batch_size=30, learning_rate_init = 0.005).fit(X_train_scaled,y_train)\n",
    "    loss_curve = MLPreg.loss_curve_\n",
    "    \n",
    "    plt.plot(np.sqrt(loss_curve))\n",
    "    plt.title(\"MLP Regressor Loss Curves\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3Qns.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
